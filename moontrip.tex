\documentclass[twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}

\title{Moontrip: notas}
\author{Ricardo Cruz}

\usepackage{hyperref}

% estilo
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=0pt}
\usepackage{indentfirst}

\usepackage{amsmath}
\newcommand{\norm}[1]{\lvert #1 \rvert}

\begin{document}

\maketitle

\section{Matriz Documento-Termo}

Uma prática comum em Text Mining é a de estruturar cada documento pela lista de palavras que o compõe. Perdemos assim informação semântica; há técnicas de NLP para trabalhar com a organização gramatical do documento, mas no geral as técnicas baseadas em contar as ocorrências de cada palavra em cada documento acabam até por ser mais bem sucedidas.

No nosso caso, pensamos usar como ``documentos'', as várias páginas da, de início, \href{http://wikipedia.org}{Wikipedia}, e, depois, \href{http://wikivoyage.org}{Wikivoyage}, para estruturar uma matriz Documento-Termo. Temos assim para cada documento, os termos associados com ele:

\noindent
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
	& Termo A & Termo B & Termo C \\\hline
	Destino A & 0 & 5 & 2 \\\hline
	Destino B & 0 & 2 & 4 \\\hline
	Destino C & 5 & 3 & 4 \\\hline
\end{tabular}
\end{center}

Normalmente, aplica-se uma transformação local a estes valores. Uma transformação local é ir, um a um, e aplicar-lhes uma função. Por exemplo, se o documento A for 5 vezes mais pequeno que o documento B, não quer dizer que o termo C aparecer mais vezes no documento B signifique que este é mais significativo. O texto em si é mais pequeno. Nestes casos, por vezes usa-se uma transformação local binária: ou seja, transforma-se em 1 se $> 0$, ou 0 se 0.

Nós usamos a função TF (term frequency), ou seja, não aplicamos nenhuma transformação local. Outras transformações possíveis, para um termo $i$ dado um documento $j$:

\noindent
\begin{center}
\begin{tabular}{|l|l|}
\hline
	Binária & $l_{ij}=1$ se existir termo, ou $0$ senão \\\hline
	TF & $l_{ij}=\mathrm{tf}_{ij}$ \\\hline
	Log & $l_{ij}=\log(\mathrm{tf}_{ij}+1)$ \\\hline
	Augnorm & $l_{ij}=\frac{\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}+1}{2}$ \\\hline
\end{tabular}
\end{center}

Para além desta transformação local (que não olha para os outros valores da matriz, é uma transformação de elemento-a-elemento) também é costume pensar-se numa transformação global. A ideia é que ao comparar documentos (ou quando um utilizador procura por vários termos), ou quando queremos dizer quais os termos mais relevantes para um dado documento, não queremos dar o mesmo peso a palavras genéricas como ``praia'' que damos a termos mais específicos como ``surf''. Se ``praia'' ocorre em muitos destinos, e ``surf'' em apenas alguns, então queremos que estes destinos de ``surf'' sejam mais semelhantes que os de ``praia''.

Nós usamos a função Idf. Outras seguem. Notação: $\mathrm{gf}_i$ denota o número de vezes que o termo $i$ ocorre em todos os documentos, e $\mathrm{df}_i$ é o número de documentos em que o termo $i$ ocorre. Na entropia, $p_{ij}=\frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}$.

\noindent
\begin{center}
\begin{tabular}{|l|l|}
\hline
	Binária & $g_i=1$ \\\hline
	Normal & $g_i=\frac{1}{\sqrt{\sum_j\mathrm{tf}_{ij}^2}}$ \\\hline
	GfIdf & $g_i=\mathrm{gf}_i/\mathrm{df}_i$ \\\hline
	Idf & $g_i=\log_2 \frac{n}{1+\mathrm{df}_i}$ \\\hline
	Entropia & $g_i=1+\sum_j \frac{p_{ij}\log p_{ij}}{\log n}$ \\\hline
\end{tabular}
\end{center}

No final, então a matriz documento-termo é computada com o produto da transformação local e da global: $l_{ij} \times g_i$. Por exemplo, no caso da transformação TF-IDF podemos ver cada documento como sendo um vector ao longo dos termos, chama-se a uso de modelo espacial de termos:

\noindent
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
	& Termo A & Termo B & Termo C \\\hline
	Destino A & 0 & 0.8 & 0.45 \\\hline
	Destino B & 0 & 0.2 & 0.9 \\\hline
	Destino C & 1 & 0.6 & 0.9 \\\hline
\end{tabular}
\end{center}

\section{Redução dos dados}

Uma técnica de redução de dados, que ainda não usamos, é a de eliminar termos que ocorrem poucas vezes, ou em demasiados documentos.

Também é feito em text mining muito uso de matrizes esparsas, uma vez que há muitos casos de zeros. São técnicas em que não são guardados todos os elementos zero, mas informações como ``os próximos 30 elementos são zeros''.

\section{Transformação Semântica}

Para além da transformação local e global, é comum também fazer-se uma chamada transformação semântica (LSA). A ideia é baseada nos algoritmos de compressão, em particular no PCA. Se sabemos que sempre que alguém diz que ``chove'' então ``cai água'', podemos condensar esta informação no nosso cérebro como ``chuva''=``cair água''. Da mesma forma, numa amostra do clima, se sabemos que sempre que chove então em 5\% dos casos há arco-íris, então se nos interessa apenas voltar a reproduzir o histograma dessa amostra, podemos esquecer os casos em que houve arco-íris e guardar apenas a informação ``chuva''=5\% arco-íris.

No caso da nossa matriz documento-termo, seria portanto possível reduzir as suas dimensões, se soubermos que sempre que ocorre o termo A, vai ocorrer $n$ vezes o termo B, pois invés de guardar a coluna do termo B, podemos apenas guardar a informação termo B = $\frac{1}{n}$ termo A, e voltar a reproduzir a coluna sempre que quisermos.

Até aqui foi uma explicação muito por alto do PCA. No caso do LSA, a ideia é usar estas técnicas não para comprimir dados, mas para encontrar relações entre dados, como muitas vezes é feito por estatísticos humanos. Executa-se um algoritmo de compressão não porque nos interessa o resultado do algoritmo, mas porque nos interessa ver que tipos de relações é que o algoritmo de compressão conseguiu encontrar. Por exemplo, conseguindo relacionar ``surf'' com ``windsurf''.

Usando esta técnica de compressão, o algoritmo LSA identifica de que forma é que os termos são semelhantes; desta forma resolve dois problemas num só: o problema de pesquisar usando sinónimos, e o problema de pesquisar usando homógrafos (palavras iguais mas com significados diferentes).

\section{Pesquisar por termos}

Normalmente são usadas abordagens muito semelhantes ao comparar documentos e ao pesquisar por termos, porque no fundo é a mesma coisa. Perguntar quais cidades são parecidas com o Porto é o mesmo que perguntar em quais se produz vinho, se fala Português, se come tripas, etc. Para todos os propósitos, quando o utilizador procura por uma série de termos é o mesmo que efectivamente inserir estes termos como um ``destino'' na matriz e comparar com os outros destinos na matriz. Devemos portanto aplicar as mesmas transformações à pesquisa do utilizador, que aplicamos aos destinos, e depois compará-los com os destinos na nossa matriz, usando algum critério de semelhança.

Uma comparação comum ao comparar duas linhas na matriz documento-termo, que é o que estamos a fazer, é a soma do produto vectorial, que é igual ao coseno: $\cos \theta = \frac{\vec{a} \cdot \vec{b}}{\norm{\vec{a}} \, \norm{\vec{b}}}$.

\begin{thebibliography}{99}
\bibitem{livro1} David Hand, Heikki Mannila e Padhraic Smyth. \href{ftp://gamma.sbin.org/pub/doc/books/Principles_of_Data_Mining.pdf}{\emph{Principles of Data Mining}}. 2001. MIT Press.
\bibitem{R:nlp} \href{http://cran.r-project.org/web/views/NaturalLanguageProcessing.html
}{Documentação dos pacotes de NLP} no CRAN.
\bibitem{resumo:lsa} Deerwester et al. \href{http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf}{Indexing by Latent Semantic Analysis}, resumo de LSA.
\bibitem{wikipedia:lsa} \href{http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management}{Latent semantic indexing} na wikipedia, tem muita informação até sobre transformações locais e globais.
\end{thebibliography}

\end{document}
